# -*- coding: utf-8 -*-
"""ppo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lLKhERZqsdRgMYUYxL-2qblxSz29RGr4
"""

!pip install gymnasium[classic-control] torch matplotlib --quiet

import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.distributions.categorical import Categorical
from tqdm.notebook import trange
from matplotlib import animation
from IPython.display import HTML, display
import os
from gymnasium.vector import SyncVectorEnv

class ActorCritic(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, 64), nn.ReLU()
        )
        self.actor = nn.Linear(64, act_dim)
        self.critic = nn.Linear(64, 1)

    def forward(self, obs):
        x = self.shared(obs)
        return self.actor(x), self.critic(x)

def make_env():
    def thunk():
        return gym.make("CartPole-v1")
    return thunk

env = SyncVectorEnv([make_env() for _ in range(8)])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

obs_dim = env.single_observation_space.shape[0]
act_dim = env.single_action_space.n
net = ActorCritic(obs_dim, act_dim).to(device)
optimizer = optim.Adam(net.parameters(), lr=1e-4)

def compute_gae(rewards, values, next_value, dones, gamma=0.99, lam=0.95):
    adv = torch.zeros_like(rewards).to(device)
    gae = 0
    for t in reversed(range(len(rewards))):
        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
        gae = delta + gamma * lam * (1 - dones[t]) * gae
        adv[t] = gae
        next_value = values[t]
    return adv

def collect_trajectory(env, net, buffer_limit):
    obs = env.reset()[0]
    done = np.zeros(env.num_envs)
    buffer = []
    steps = 0
    while steps < buffer_limit:
        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)
        logits, value = net(obs_tensor)
        probs = Categorical(logits=logits)
        action = probs.sample()
        log_prob = probs.log_prob(action)

        next_obs, reward, terminated, truncated, _ = env.step(action.cpu().numpy())
        done_flag = np.logical_or(terminated, truncated)

        for i in range(env.num_envs):
            buffer.append((obs[i], action[i].item(), log_prob[i].item(), reward[i], value[i].item(), done_flag[i]))

        obs = next_obs
        done = done_flag
        steps += env.num_envs
    return buffer, obs

def update_policy(buffer, last_obs, net, optimizer, update_epochs, minibatch_size, clip_eps, entropy_coef, value_coef):
    obs_list, act_list, logp_list, rew_list, val_list, done_list = zip(*buffer)

    obs_tensor = torch.tensor(obs_list, dtype=torch.float32).to(device)
    act_tensor = torch.tensor(act_list).to(device)
    logp_tensor = torch.tensor(logp_list).to(device)
    val_tensor = torch.tensor(val_list, dtype=torch.float32).to(device)
    rew_tensor = torch.tensor(rew_list, dtype=torch.float32).to(device)
    done_tensor = torch.tensor(done_list, dtype=torch.float32).to(device)

    with torch.no_grad():
        last_obs_tensor = torch.tensor(last_obs, dtype=torch.float32).to(device)
        _, next_value = net(last_obs_tensor)
        next_value = next_value.mean(dim=0)

    adv_tensor = compute_gae(rew_tensor, val_tensor, next_value, done_tensor)
    returns_tensor = adv_tensor + val_tensor
    adv_tensor = (adv_tensor - adv_tensor.mean()) / (adv_tensor.std() + 1e-8)

    for _ in range(update_epochs):
        idx = torch.randperm(len(obs_tensor))
        for i in range(0, len(obs_tensor), minibatch_size):
            batch_idx = idx[i:i+minibatch_size]
            batch_obs = obs_tensor[batch_idx]
            batch_act = act_tensor[batch_idx]
            batch_adv = adv_tensor[batch_idx]
            batch_ret = returns_tensor[batch_idx]
            batch_logp_old = logp_tensor[batch_idx]
            batch_val_old = val_tensor[batch_idx]

            logits, values = net(batch_obs)
            probs = Categorical(logits=logits)
            new_logp = probs.log_prob(batch_act)
            entropy = probs.entropy().mean()

            ratio = (new_logp - batch_logp_old).exp()
            surr1 = ratio * batch_adv
            surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * batch_adv
            policy_loss = -torch.min(surr1, surr2).mean()

            value_pred_clipped = batch_val_old + (values.squeeze() - batch_val_old).clamp(-clip_eps, clip_eps)
            value_loss1 = (values.squeeze() - batch_ret).pow(2)
            value_loss2 = (value_pred_clipped - batch_ret).pow(2)
            value_loss = 0.5 * torch.max(value_loss1, value_loss2).mean()

            loss = policy_loss + value_coef * value_loss - entropy_coef * entropy

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

num_episodes = 2000
minibatch_size = 64
update_epochs = 4
clip_eps = 0.2
entropy_init = 0.02
value_coef = 0.5
reward_smoothing_alpha = 0.1
best_reward = -np.inf
rewards_history = []
smoothed_reward = 0
buffer_limit = 2048

def plot_metrics():
    plt.figure(figsize=(12, 6))
    plt.plot(rewards_history, label="Episode reward", alpha=0.4)
    ema_rewards = []
    ema = 0
    alpha = 0.1
    for r in rewards_history:
        ema = alpha * r + (1 - alpha) * ema
        ema_rewards.append(ema)
    plt.plot(ema_rewards, label="EMA Reward", color='orange')
    if len(rewards_history) >= 100:
        rolling_success = [np.mean(np.array(rewards_history[i:i+100]) >= 200) for i in range(len(rewards_history)-99)]
        plt.plot(range(99, len(rewards_history)), rolling_success, label="Success Rate (100ep window)", color='green')
    plt.xlabel("Episode")
    plt.ylabel("Reward / Success")
    plt.title("PPO Training Progress")
    plt.legend()
    plt.grid(True)
    plt.show()

for episode in trange(num_episodes):
    buffer, last_obs = collect_trajectory(env, net, buffer_limit)
    ep_reward = sum([transition[3] for transition in buffer]) / env.num_envs

    entropy_coef = max(0.001, entropy_init * (1 - episode / num_episodes))

    update_policy(buffer, last_obs, net, optimizer, update_epochs, minibatch_size, clip_eps, entropy_coef, value_coef)

    rewards_history.append(ep_reward)
    smoothed_reward = reward_smoothing_alpha * ep_reward + (1 - reward_smoothing_alpha) * smoothed_reward
    if smoothed_reward > best_reward:
        best_reward = smoothed_reward
        torch.save(net.state_dict(), "best_ppo_cartpole.pth")

plot_metrics()

def render_agent_as_gif(net, env, max_frames=500):
    frames = []
    obs, _ = env.reset()
    done = False

    for _ in range(max_frames):
        frame = env.render()
        frames.append(frame)

        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            logits, _ = net(obs_tensor)
            probs = Categorical(logits=logits)
            action = probs.sample().item()

        obs, reward, terminated, truncated, _ = env.step(action)
        if terminated or truncated:
            break

    env.close()
    fig = plt.figure(figsize=(frames[0].shape[1] / 72, frames[0].shape[0] / 72), dpi=72)
    plt.axis("off")
    im = plt.imshow(frames[0])

    def update(frame):
        im.set_array(frame)
        return [im]

    ani = animation.FuncAnimation(fig, update, frames=frames, interval=30)
    html = ani.to_jshtml()
    plt.close()
    display(HTML(html))

render_env = gym.make("CartPole-v1", render_mode="rgb_array")
net.load_state_dict(torch.load("best_ppo_cartpole.pth"))
render_agent_as_gif(net, render_env)